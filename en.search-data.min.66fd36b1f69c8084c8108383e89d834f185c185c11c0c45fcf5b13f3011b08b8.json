[{"id":0,"href":"/notes/docs/ml-system/papers/clipper/","title":"Clipper","section":"Papers","content":"Clipper: A Low-Latency Online Prediction Serving System #  Paper link.\nThis paper presents a general ML inference framework (developed side by side with TF Serving).\n"},{"id":1,"href":"/notes/docs/ml-system/papers/gandiva/","title":"Gandiva","section":"Papers","content":"Gandiva: Introspective Cluster Scheduling for Deep Learning #  Paper link.\nTODO\n"},{"id":2,"href":"/notes/docs/ml-system/papers/infaas/","title":"INFaaS","section":"Papers","content":"INFaaS: A Model-less and Managed Inference Serving System #  Paper link.\nThis paper proposes a way to construct and search for model-variants such that they meet certain SLO.\n"},{"id":3,"href":"/notes/docs/ml-system/papers/pollux/","title":"Pollux","section":"Papers","content":"Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning #  Paper link.\nTODO\n"},{"id":4,"href":"/notes/docs/ml-system/papers/tf-federated/","title":"TF-Federated","section":"Papers","content":"TOWARDS FEDERATED LEARNING AT SCALE: SYSTEM DESIGN #  Paper link.\nThis is a really nice comic about federated learning.\nFL focuses on synchronous approaches. This system enables one to train a deep neural network, using TensorFlow, on data stored on the phone which will never leave the device.\nFederated-learning protocol:\nDevices announce to the server that they are ready to run an FL task for a given FL population. An FL population is specified by a globally unique name which identifies the learning problem, or application, which is worked upon.  How often does devices communicate with servers for LF tasks? Does many FL populations affect server searching latency? KV store problem?    The server sends a FL plan (graph and execution instruction) and FL checkpoint (states) to devices. Devices only sends FL checkpoints back to servers.\nDuring device selection phase, eligible devices (charing, unmetered network) periodically check in with server by opening a bidirectional channel. The channel is used for liveness and communication.  Is TCP used? Is it a suitable fit? Current implementation uses random sampling, which can be improved to address selection bias. Sampling seems to be performed on the server side. If a device has sensitive data, should the device handle most selection bias logic and only inform servers, as opposed to letting server handle selection bias?    Since server only performs aggregation of models, could we accelerate aggregation on SmartNIC or programmable switches? Does model aggregation require complex matrix multiplication? Does batch make sense for aggregation?  The server waits for the participating devices to report updates. As updates are received, the server aggregates them using Federated Averaging and instructs the reporting devices when to reconnect (see also Sec. 2.3). If enough devices report in time, the round will be success- fully completed and the server will update its global model, otherwise the round is abandoned.  Does round based updating make sense? If the server has to wait for all device participants for updates, this process is likely far slower than data-center training where you have thousands of GPUs connected together through high-bandwidth \u0026amp; low-latency networks. The advantage is obviously privacy. But could we use the bandwidth-to-hide-latency approach to compensate for distributed training by having far more devices?    straggling devices which do not report back in time or do not react on configuration by the server will simply be ignored. The protocol has a certain tolerance for such drop-outs which is configurable per FL task.  How much does the drop-out threshold affect model accuracy? Could we dynamically adjust it depending on network condition?    Server #  Server rely on actor model.\nA different training \u0026amp; serving framework just for FL?  Instances of actors placement policy. How long does a training instance usually last? Long or short?  Actors in FL Server arch:\nDoes all members need to participate in lockstep? Can we free them as soon as they finish their job? E.g. when selector finishes communicating with all devices, could we release them by using the fact that (possibly) each round takes a long time to train?  An FL plan consists of two parts: one for the device and one for the server. The device portion of the FL plan contains, among other things: the TensorFlow graph itself, selection criteria for training data in the example store, instructions on how to batch data and how many epochs to run on the device, labels for the nodes in the graph which represent certain computations like loading and saving weights, and so on.  Hyperparameter tuning, but on device. So we don\u0026rsquo;t have luxury for large scale tuning.    "}]